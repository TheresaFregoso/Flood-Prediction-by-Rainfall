{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Flood Potential Based on Rain Fall for the San Lorenzo River Basin, California"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ETL Step 1:  \n",
    "- Extracting data from Stream Gage USGS 11160500 San Lorenzo at Big Trees for the time period 09/01/2014 to 09/01/2024.\n",
    "\n",
    "  Stream Gage data was obtained through download via the [USGS for station 11160500]. River height in feet, and stream flow in cubic feet per second, measured every 15 minutes.\n",
    "\n",
    "- Extracting precipitation data relative to the San Lorenzo River Watershed for the same time period, 09/01/2014 to 09/01/2024.\n",
    "  Data was obtained from the [California Data Exchange Center, Department of Water Sources site]\n",
    "  \n",
    "  4 stations were identified that have hourly data readings in inches of rainfall:\n",
    "\n",
    "| Location            | Code | Elevation | Latitude  | Longitude    | County     | Agency                              |\n",
    "|---------------------|------|-----------|-----------|--------------|------------|-------------------------------------|\n",
    "| BEN LOMOND (CDF)     | BLO  | 2630      | 37.132000 | -122.169998  | SANTA CRUZ | CA Dept of Forestry and Fire Protection |\n",
    "| SCHULTIES RD         | SCH  | 1400      | 37.132999 | -121.969002  | SANTA CRUZ | Santa Cruz County                   |\n",
    "| BOULDER CREEK        | BDC  | 800       | 37.141998 | -122.163002  | SANTA CRUZ | Santa Cruz County                   |\n",
    "| BEN LOMOND           | BLN  | 365       | 37.092999 | -122.074997  | SANTA CRUZ | Santa Cruz County                   |\n",
    "\n",
    "BLO is the only station that has continuous measurements, the other three stations have event based measurements that are recorded based on a tipping point.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[USGS for station 11160500]: https://waterdata.usgs.gov/monitoring-location/11160500/#parameterCode=00065&period=P7D&showMedian=false\n",
    "[California Data Exchange Center, Department of Water Sources site]: https://cdec.water.ca.gov/dynamicapp/wsSensorData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the Stream Gage data to a Pandas data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  agency   site_no            datetime time_zone  gage_height approval_code\n",
      "0   USGS  11160500 2014-09-01 00:00:00       PDT         2.69             A\n",
      "1   USGS  11160500 2014-09-01 00:15:00       PDT         2.69             A\n",
      "2   USGS  11160500 2014-09-01 00:30:00       PDT         2.69             A\n",
      "3   USGS  11160500 2014-09-01 00:45:00       PDT         2.69             A\n",
      "4   USGS  11160500 2014-09-01 01:00:00       PDT         2.69             A\n"
     ]
    }
   ],
   "source": [
    "# Define the file path \n",
    "file_path = 'Resources/BigTrees11160500_9_2014_9_2024.txt'\n",
    "\n",
    "# Skip the header rows and load the data into a DataFrame\n",
    "stream = pd.read_csv(file_path, sep='\\t', comment='#', skiprows=28, header=0)\n",
    "\n",
    "# Rename the columns\n",
    "stream.columns = ['agency', 'site_no', 'datetime', 'time_zone', 'gage_height', 'approval_code']\n",
    "\n",
    "# Convert the 'datetime' column to datetime type for easier manipulation\n",
    "stream['datetime'] = pd.to_datetime(stream['datetime'])\n",
    "\n",
    "# Display the DataFrame\n",
    "print(stream.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to Resources/transformed_stream_gage_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Define the path for the new CSV output\n",
    "output_csv = 'Resources/transformed_stream_gage_data.csv'\n",
    "\n",
    "# Save the DataFrame to a new CSV file\n",
    "stream.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"Data has been saved to {output_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform date to have only one measurement per hour to match the rain data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            date_hour  gage_height\n",
      "0 2014-09-01 00:00:00         2.69\n",
      "1 2014-09-01 01:00:00         2.69\n",
      "2 2014-09-01 02:00:00         2.69\n",
      "3 2014-09-01 03:00:00         2.68\n",
      "4 2014-09-01 04:00:00         2.68\n",
      "Data has been saved to Resources/transformed_stream_gage_data_hourly.csv\n"
     ]
    }
   ],
   "source": [
    "# Convert the 'datetime' column to datetime format if it exists\n",
    "if 'datetime' in stream.columns:\n",
    "    stream['datetime'] = pd.to_datetime(stream['datetime'], errors='coerce')\n",
    "\n",
    "# Extract the date and hour from the 'datetime' column\n",
    "stream['date_hour'] = stream['datetime'].dt.floor('h')\n",
    "\n",
    "# Group by the date and hour and get the max gage height for each hour\n",
    "max_height_per_hour = stream.groupby('date_hour').agg({'gage_height': 'max'}).reset_index()\n",
    "\n",
    "# Display the new DataFrame with the date, hour, and max height\n",
    "print(max_height_per_hour.head())\n",
    "\n",
    "# Define the path for the new CSV output\n",
    "output_csv = 'Resources/transformed_stream_gage_data_hourly.csv'\n",
    "\n",
    "# Save the DataFrame to a new CSV file\n",
    "max_height_per_hour.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"Data has been saved to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeating the same process for the river flow data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  agency   site_no            datetime time_zone  discharge approval_code\n",
      "0   USGS  11160500 2014-09-01 00:00:00       PDT       7.09             A\n",
      "1   USGS  11160500 2014-09-01 00:15:00       PDT       7.09             A\n",
      "2   USGS  11160500 2014-09-01 00:30:00       PDT       7.09             A\n",
      "3   USGS  11160500 2014-09-01 00:45:00       PDT       7.09             A\n",
      "4   USGS  11160500 2014-09-01 01:00:00       PDT       7.09             A\n",
      "Data has been saved to Resources/transformed_stream_discharge_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Define the file path \n",
    "file_path = 'Resources/BigTrees11160500_9_2014_9_2024_flow.txt'\n",
    "\n",
    "# Skip the header rows and load the data into a DataFrame\n",
    "# Skip rows with the description of column widths like '5s', '15s', etc.\n",
    "stream_flow = pd.read_csv(file_path, sep='\\t', comment='#', skiprows=[28, 29], header=0, dtype={'site_no': str, '14696_00060': float})\n",
    "\n",
    "# Rename the columns for easier reference\n",
    "stream_flow.columns = ['agency', 'site_no', 'datetime', 'time_zone', 'discharge', 'approval_code']\n",
    "\n",
    "# Convert the 'datetime' column to datetime type for easier manipulation, ignoring errors\n",
    "stream_flow['datetime'] = pd.to_datetime(stream['datetime'], errors='coerce')\n",
    "\n",
    "# Display the DataFrame (first few rows)\n",
    "print(stream_flow.head())\n",
    "\n",
    "# Define the path for the new CSV output\n",
    "output_csv = 'Resources/transformed_stream_discharge_data.csv'\n",
    "\n",
    "# Save the DataFrame to a new CSV file\n",
    "stream_flow.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"Data has been saved to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the Max flow value per hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            date_hour  discharge\n",
      "0 2014-09-01 00:00:00       7.09\n",
      "1 2014-09-01 01:00:00       7.09\n",
      "2 2014-09-01 02:00:00       7.09\n",
      "3 2014-09-01 03:00:00       6.86\n",
      "4 2014-09-01 04:00:00       6.86\n",
      "Data has been saved to Resources/transformed_max_flow_data_hourly.csv\n"
     ]
    }
   ],
   "source": [
    "# Convert the 'datetime' column to datetime format if it exists\n",
    "if 'datetime' in stream_flow.columns:\n",
    "    stream_flow['datetime'] = pd.to_datetime(stream_flow['datetime'], errors='coerce')\n",
    "\n",
    "# Extract the date and hour from the 'datetime' column\n",
    "stream_flow['date_hour'] = stream_flow['datetime'].dt.floor('h')\n",
    "\n",
    "# Group by the date and hour and get the max discharge for each hour\n",
    "max_flow_per_hour = stream_flow.groupby('date_hour').agg({'discharge': 'max'}).reset_index()\n",
    "\n",
    "# Display the new DataFrame with the date, hour, and max discharge\n",
    "print(max_flow_per_hour.head())\n",
    "\n",
    "# Define the path for the new CSV output\n",
    "output_csv = 'Resources/transformed_max_flow_data_hourly.csv'\n",
    "\n",
    "# Save the DataFrame to a new CSV file\n",
    "max_flow_per_hour.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"Data has been saved to {output_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting rain data to Pandas data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniforge/base/envs/flooding_model/lib/python3.11/site-packages/openpyxl/styles/stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Sheet1']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Excel file\n",
    "file_path = 'Resources/BLO_2.xlsx'\n",
    "\n",
    "# Reading the Excel file to inspect sheet names and general structure\n",
    "xls = pd.ExcelFile(file_path)\n",
    "\n",
    "# Display the sheet names to understand how the data is organized\n",
    "xls.sheet_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniforge/base/envs/flooding_model/lib/python3.11/site-packages/openpyxl/styles/stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATION_ID</th>\n",
       "      <th>DURATION</th>\n",
       "      <th>SENSOR_NUMBER</th>\n",
       "      <th>SENS_TYPE</th>\n",
       "      <th>OBS DATE</th>\n",
       "      <th>VALUE</th>\n",
       "      <th>DATA_FLAG</th>\n",
       "      <th>UNITS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BLO</td>\n",
       "      <td>H</td>\n",
       "      <td>2</td>\n",
       "      <td>RAIN</td>\n",
       "      <td>2014-09-01 00:00:00</td>\n",
       "      <td>0.04</td>\n",
       "      <td></td>\n",
       "      <td>INCHES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BLO</td>\n",
       "      <td>H</td>\n",
       "      <td>2</td>\n",
       "      <td>RAIN</td>\n",
       "      <td>2014-09-01 01:00:00</td>\n",
       "      <td>0.04</td>\n",
       "      <td></td>\n",
       "      <td>INCHES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BLO</td>\n",
       "      <td>H</td>\n",
       "      <td>2</td>\n",
       "      <td>RAIN</td>\n",
       "      <td>2014-09-01 02:00:00</td>\n",
       "      <td>0.04</td>\n",
       "      <td></td>\n",
       "      <td>INCHES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BLO</td>\n",
       "      <td>H</td>\n",
       "      <td>2</td>\n",
       "      <td>RAIN</td>\n",
       "      <td>2014-09-01 03:00:00</td>\n",
       "      <td>0.04</td>\n",
       "      <td></td>\n",
       "      <td>INCHES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BLO</td>\n",
       "      <td>H</td>\n",
       "      <td>2</td>\n",
       "      <td>RAIN</td>\n",
       "      <td>2014-09-01 04:00:00</td>\n",
       "      <td>0.04</td>\n",
       "      <td></td>\n",
       "      <td>INCHES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  STATION_ID DURATION  SENSOR_NUMBER SENS_TYPE            OBS DATE  VALUE  \\\n",
       "0        BLO        H              2      RAIN 2014-09-01 00:00:00   0.04   \n",
       "1        BLO        H              2      RAIN 2014-09-01 01:00:00   0.04   \n",
       "2        BLO        H              2      RAIN 2014-09-01 02:00:00   0.04   \n",
       "3        BLO        H              2      RAIN 2014-09-01 03:00:00   0.04   \n",
       "4        BLO        H              2      RAIN 2014-09-01 04:00:00   0.04   \n",
       "\n",
       "  DATA_FLAG   UNITS  \n",
       "0            INCHES  \n",
       "1            INCHES  \n",
       "2            INCHES  \n",
       "3            INCHES  \n",
       "4            INCHES  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since the file contains a single sheet 'Sheet1', let's load it and inspect the first few rows to understand the data structure.\n",
    "rain_BLO = pd.read_excel(file_path, sheet_name='Sheet1')\n",
    "\n",
    "# Drop the 'datetime' column\n",
    "rain_BLO = rain_BLO.drop(columns=['DATE TIME'])\n",
    "\n",
    "# Convert the 'obs_date' column to datetime\n",
    "rain_BLO['OBS DATE'] = pd.to_datetime(rain['OBS DATE'], errors='coerce')\n",
    "\n",
    "# Display the first few rows of the DataFrame to understand the structure\n",
    "rain_BLO.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to Resources/transformed_BLO_rain_data_hourly.csv\n"
     ]
    }
   ],
   "source": [
    "# Define the path for the new CSV output\n",
    "output_csv = 'Resources/transformed_BLO_rain_data_hourly.csv'\n",
    "\n",
    "# Save the DataFrame to a new CSV file\n",
    "rain.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"Data has been saved to {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATION_ID               object\n",
      "DURATION                 object\n",
      "SENSOR_NUMBER             int64\n",
      "SENS_TYPE                object\n",
      "OBS DATE         datetime64[ns]\n",
      "VALUE                   float64\n",
      "DATA_FLAG                object\n",
      "UNITS                    object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(rain_BLO.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and clean Rain data for stations BDC, BLN, and SCH that only record with tipping point and compare with Daily measured rainfall to check for errors  \n",
    "\n",
    "A sensor type where the field measuring device uses a calibrated bucket that tips when full of precipitation. The amount of precipitation at which the device tips is usually 0.04 inches. There are some gages that tip at 0.01 inches. The value usually accumulates or gets larger until it is reset. A reset may occur if a technician visits the site or it is near the beginning of the season. The dates that designate a season varies according to different agencies (ie. July-June, October-September). Generally, this sensor type is used for real-time collection duration of hourly or event data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BDC Station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniforge/base/envs/flooding_model/lib/python3.11/site-packages/openpyxl/styles/stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n",
      "/usr/local/Caskroom/miniforge/base/envs/flooding_model/lib/python3.11/site-packages/openpyxl/styles/stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Date  Calculated Cumulative Rainfall (inches)  \\\n",
      "0     2015-10-23                                      0.0   \n",
      "1     2015-10-24                                      0.0   \n",
      "2     2015-10-25                                      0.0   \n",
      "3     2015-10-26                                      0.0   \n",
      "4     2015-10-27                                      0.0   \n",
      "...          ...                                      ...   \n",
      "2629  2024-08-16                                      0.0   \n",
      "2630  2024-08-19                                      0.0   \n",
      "2631  2024-08-23                                      0.0   \n",
      "2632  2024-08-26                                      0.0   \n",
      "2633  2024-08-31                                      0.0   \n",
      "\n",
      "      Daily Rainfall (inches)  Discrepancy  \n",
      "0                         0.0        False  \n",
      "1                         0.0        False  \n",
      "2                         0.0        False  \n",
      "3                         0.0        False  \n",
      "4                         0.0        False  \n",
      "...                       ...          ...  \n",
      "2629                      0.0        False  \n",
      "2630                      0.0        False  \n",
      "2631                      0.0        False  \n",
      "2632                      0.0        False  \n",
      "2633                      0.0        False  \n",
      "\n",
      "[2634 rows x 4 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_d/q6dl733j0hdfhxbg883rg3xc0000gn/T/ipykernel_2662/1403092099.py:9: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  file_16['Rounded Date'] = file_16['OBS DATE'].dt.round('H')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the two Excel files\n",
    "file_16 = pd.read_excel('Resources/BDC_16.xlsx')\n",
    "file_2 = pd.read_excel('Resources/BDC_2.xlsx')\n",
    "\n",
    "# Convert 'OBS DATE' in file_16 to datetime and round to nearest hour\n",
    "file_16['OBS DATE'] = pd.to_datetime(file_16['OBS DATE'])\n",
    "file_16['Rounded Date'] = file_16['OBS DATE'].dt.round('H')\n",
    "\n",
    "# Calculate rainfall increments for file 16\n",
    "file_16['Rainfall'] = file_16['VALUE'].diff().fillna(0)  # Calculate rainfall increments\n",
    "\n",
    "# Calculate daily cumulative rainfall for File 16\n",
    "file_16_daily = file_16.groupby(file_16['Rounded Date'].dt.date)['Rainfall'].sum().reset_index()\n",
    "file_16_daily.columns = ['Date', 'Calculated Cumulative Rainfall (inches)']\n",
    "\n",
    "# Convert 'OBS DATE' in file_2 to datetime\n",
    "file_2['OBS DATE'] = pd.to_datetime(file_2['OBS DATE'])\n",
    "\n",
    "# Extract relevant daily cumulative data from file 2\n",
    "file_2_daily = file_2[['OBS DATE', 'VALUE']].dropna()\n",
    "file_2_daily.columns = ['Date', 'Actual Cumulative Rainfall (inches)']\n",
    "\n",
    "# Calculate daily rainfall for File 2\n",
    "file_2_daily['Daily Rainfall (inches)'] = file_2_daily['Actual Cumulative Rainfall (inches)'].diff().fillna(0)\n",
    "\n",
    "# Handle reset condition: if current day value is less than the previous day's value, it's a reset\n",
    "file_2_daily.loc[file_2_daily['Actual Cumulative Rainfall (inches)'] < file_2_daily['Actual Cumulative Rainfall (inches)'].shift(1), \n",
    "                 'Daily Rainfall (inches)'] = file_2_daily['Actual Cumulative Rainfall (inches)']\n",
    "\n",
    "# Ensure both 'Date' columns are of the same type\n",
    "file_2_daily['Date'] = file_2_daily['Date'].dt.date\n",
    "\n",
    "# Merge the two datasets on 'Date'\n",
    "merged_data = pd.merge(file_16_daily, file_2_daily[['Date', 'Daily Rainfall (inches)']], on='Date', how='inner')\n",
    "\n",
    "# Step 4: Identify discrepancies\n",
    "merged_data['Discrepancy'] = merged_data['Calculated Cumulative Rainfall (inches)'] != merged_data['Daily Rainfall (inches)']\n",
    "\n",
    "# Display the merged data with calculated discrepancies\n",
    "print(merged_data)\n",
    "\n",
    "merged_data.to_csv('Resources/BDC_Daily_Rainfall_compare.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove data from 16 file if discrepancy is greater than .25 inches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      STATION_ID DURATION  SENSOR_NUMBER SENS_TYPE         DATE TIME  \\\n",
      "0            BDC        E             16   RAINTIP  2015-10-23 00:52   \n",
      "1            BDC        E             16   RAINTIP  2015-10-23 12:52   \n",
      "2            BDC        E             16   RAINTIP  2015-10-24 00:52   \n",
      "3            BDC        E             16   RAINTIP  2015-10-25 00:52   \n",
      "4            BDC        E             16   RAINTIP  2015-10-25 12:52   \n",
      "...          ...      ...            ...       ...               ...   \n",
      "11429        BDC        E             16   RAINTIP  2024-08-19 01:49   \n",
      "11430        BDC        E             16   RAINTIP  2024-08-19 13:49   \n",
      "11431        BDC        E             16   RAINTIP  2024-08-23 13:49   \n",
      "11432        BDC        E             16   RAINTIP  2024-08-26 13:49   \n",
      "11433        BDC        E             16   RAINTIP  2024-08-31 13:49   \n",
      "\n",
      "                 OBS DATE  VALUE DATA_FLAG   UNITS        Rounded Date  \\\n",
      "0     2015-10-23 00:52:00   0.31            INCHES 2015-10-23 01:00:00   \n",
      "1     2015-10-23 12:52:00   0.31            INCHES 2015-10-23 13:00:00   \n",
      "2     2015-10-24 00:52:00   0.31            INCHES 2015-10-24 01:00:00   \n",
      "3     2015-10-25 00:52:00   0.31            INCHES 2015-10-25 01:00:00   \n",
      "4     2015-10-25 12:52:00   0.31            INCHES 2015-10-25 13:00:00   \n",
      "...                   ...    ...       ...     ...                 ...   \n",
      "11429 2024-08-19 01:49:00  53.37            INCHES 2024-08-19 02:00:00   \n",
      "11430 2024-08-19 13:49:00  53.37            INCHES 2024-08-19 14:00:00   \n",
      "11431 2024-08-23 13:49:00  53.37            INCHES 2024-08-23 14:00:00   \n",
      "11432 2024-08-26 13:49:00  53.37            INCHES 2024-08-26 14:00:00   \n",
      "11433 2024-08-31 13:49:00  53.37            INCHES 2024-08-31 14:00:00   \n",
      "\n",
      "       Rainfall  Cumulative Daily Rainfall  Total Daily Rainfall  \n",
      "0           0.0                        0.0                   0.0  \n",
      "1           0.0                        0.0                   0.0  \n",
      "2           0.0                        0.0                   0.0  \n",
      "3           0.0                        0.0                   0.0  \n",
      "4           0.0                        0.0                   0.0  \n",
      "...         ...                        ...                   ...  \n",
      "11429       0.0                        0.0                   0.0  \n",
      "11430       0.0                        0.0                   0.0  \n",
      "11431       0.0                        0.0                   0.0  \n",
      "11432       0.0                        0.0                   0.0  \n",
      "11433       0.0                        0.0                   0.0  \n",
      "\n",
      "[11434 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the absolute difference between calculated and actual rainfall\n",
    "merged_data['Difference'] = abs(merged_data['Calculated Cumulative Rainfall (inches)'] - merged_data['Daily Rainfall (inches)'])\n",
    "\n",
    "# Filter out days where the difference is greater than 0.25 inches\n",
    "filtered_data = merged_data[merged_data['Difference'] <= 0.25]\n",
    "\n",
    "# Get the dates to keep from the filtered dataset\n",
    "dates_to_keep = filtered_data['Date'].unique()\n",
    "\n",
    "# Convert 'OBS DATE' in file_16 to datetime and ensure it's compatible for filtering\n",
    "file_16['Date'] = file_16['OBS DATE'].dt.date\n",
    "\n",
    "# Filter the original 16 file to keep only rows that match the dates in 'dates_to_keep'\n",
    "filtered_file_16 = file_16[file_16['Date'].isin(dates_to_keep)].copy()  # .copy() to avoid SettingWithCopyWarning\n",
    "\n",
    "# Now, calculate cumulative hourly rainfall for each day in the filtered 16 file\n",
    "filtered_file_16['Cumulative Daily Rainfall'] = filtered_file_16.groupby('Date')['Rainfall'].cumsum()\n",
    "\n",
    "# Merge the daily rainfall back into the filtered file\n",
    "daily_totals = filtered_file_16.groupby('Date')['Rainfall'].sum().reset_index()\n",
    "daily_totals.columns = ['Date', 'Total Daily Rainfall']\n",
    "\n",
    "# Merge the daily totals into the filtered 16 file\n",
    "filtered_file_16 = pd.merge(filtered_file_16, daily_totals, on='Date')\n",
    "\n",
    "# Drop the additional 'Date' column after filtering if needed\n",
    "filtered_file_16 = filtered_file_16.drop(columns=['Date'])\n",
    "\n",
    "# Set 'Rainfall' and 'Cumulative Daily Rainfall' to 0 if 'Total Daily Rainfall' is 0\n",
    "filtered_file_16.loc[filtered_file_16['Total Daily Rainfall'] == 0, ['Rainfall', 'Cumulative Daily Rainfall']] = 0\n",
    "\n",
    "# Save the result to a CSV file\n",
    "filtered_file_16.to_csv('Resources/BDC_cleaned_hourly_Rainfall.csv', index=False)\n",
    "\n",
    "\n",
    "# Display the final dataset with cumulative hourly rainfall and total daily rainfall\n",
    "print(filtered_file_16)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BLN Station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniforge/base/envs/flooding_model/lib/python3.11/site-packages/openpyxl/styles/stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n",
      "/usr/local/Caskroom/miniforge/base/envs/flooding_model/lib/python3.11/site-packages/openpyxl/styles/stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Date  Calculated Cumulative Rainfall (inches)  \\\n",
      "0     2015-10-16                                      0.0   \n",
      "1     2015-10-17                                      0.0   \n",
      "2     2015-10-18                                      0.0   \n",
      "3     2015-10-19                                      0.0   \n",
      "4     2015-10-20                                      0.0   \n",
      "...          ...                                      ...   \n",
      "2296  2024-08-25                                      0.0   \n",
      "2297  2024-08-26                                      0.0   \n",
      "2298  2024-08-27                                      0.0   \n",
      "2299  2024-08-29                                      0.0   \n",
      "2300  2024-08-30                                      0.0   \n",
      "\n",
      "      Daily Rainfall (inches)  Discrepancy  \n",
      "0                         0.0        False  \n",
      "1                         0.0        False  \n",
      "2                         0.0        False  \n",
      "3                         0.0        False  \n",
      "4                         0.0        False  \n",
      "...                       ...          ...  \n",
      "2296                      0.0        False  \n",
      "2297                      0.0        False  \n",
      "2298                      0.0        False  \n",
      "2299                      0.0        False  \n",
      "2300                      0.0        False  \n",
      "\n",
      "[2301 rows x 4 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_d/q6dl733j0hdfhxbg883rg3xc0000gn/T/ipykernel_2662/2137034109.py:9: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  file_16['Rounded Date'] = file_16['OBS DATE'].dt.round('H')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the two Excel files\n",
    "file_16 = pd.read_excel('Resources/BLN_16.xlsx')\n",
    "file_2 = pd.read_excel('Resources/BLN_2.xlsx')\n",
    "\n",
    "# Convert 'OBS DATE' in file_16 to datetime and round to nearest hour\n",
    "file_16['OBS DATE'] = pd.to_datetime(file_16['OBS DATE'])\n",
    "file_16['Rounded Date'] = file_16['OBS DATE'].dt.round('H')\n",
    "\n",
    "# Calculate rainfall increments for file 16\n",
    "file_16['Rainfall'] = file_16['VALUE'].diff().fillna(0)  # Calculate rainfall increments\n",
    "\n",
    "# Calculate daily cumulative rainfall for File 16\n",
    "file_16_daily = file_16.groupby(file_16['Rounded Date'].dt.date)['Rainfall'].sum().reset_index()\n",
    "file_16_daily.columns = ['Date', 'Calculated Cumulative Rainfall (inches)']\n",
    "\n",
    "# Convert 'OBS DATE' in file_2 to datetime\n",
    "file_2['OBS DATE'] = pd.to_datetime(file_2['OBS DATE'])\n",
    "\n",
    "# Extract relevant daily cumulative data from file 2\n",
    "file_2_daily = file_2[['OBS DATE', 'VALUE']].dropna()\n",
    "file_2_daily.columns = ['Date', 'Actual Cumulative Rainfall (inches)']\n",
    "\n",
    "# Calculate daily rainfall for File 2\n",
    "file_2_daily['Daily Rainfall (inches)'] = file_2_daily['Actual Cumulative Rainfall (inches)'].diff().fillna(0)\n",
    "\n",
    "# Handle reset condition: if current day value is less than the previous day's value, it's a reset\n",
    "file_2_daily.loc[file_2_daily['Actual Cumulative Rainfall (inches)'] < file_2_daily['Actual Cumulative Rainfall (inches)'].shift(1), \n",
    "                 'Daily Rainfall (inches)'] = file_2_daily['Actual Cumulative Rainfall (inches)']\n",
    "\n",
    "# Ensure both 'Date' columns are of the same type\n",
    "file_2_daily['Date'] = file_2_daily['Date'].dt.date\n",
    "\n",
    "# Merge the two datasets on 'Date'\n",
    "merged_data = pd.merge(file_16_daily, file_2_daily[['Date', 'Daily Rainfall (inches)']], on='Date', how='inner')\n",
    "\n",
    "# Step 4: Identify discrepancies\n",
    "merged_data['Discrepancy'] = merged_data['Calculated Cumulative Rainfall (inches)'] != merged_data['Daily Rainfall (inches)']\n",
    "\n",
    "# Display the merged data with calculated discrepancies\n",
    "print(merged_data)\n",
    "\n",
    "merged_data.to_csv('Resources/BLN_Daily_Rainfall_compare.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove data from 16 file if discrepancy is greater than .25 inches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      STATION_ID DURATION  SENSOR_NUMBER SENS_TYPE         DATE TIME  \\\n",
      "0            BLN        E             16   RAINTIP  2015-10-16 11:08   \n",
      "1            BLN        E             16   RAINTIP  2015-10-16 23:08   \n",
      "2            BLN        E             16   RAINTIP  2015-10-17 11:08   \n",
      "3            BLN        E             16   RAINTIP  2015-10-17 23:08   \n",
      "4            BLN        E             16   RAINTIP  2015-10-18 11:08   \n",
      "...          ...      ...            ...       ...               ...   \n",
      "10600        BLN        E             16   RAINTIP  2024-08-25 12:03   \n",
      "10601        BLN        E             16   RAINTIP  2024-08-26 12:03   \n",
      "10602        BLN        E             16   RAINTIP  2024-08-27 12:03   \n",
      "10603        BLN        E             16   RAINTIP  2024-08-29 00:03   \n",
      "10604        BLN        E             16   RAINTIP  2024-08-30 12:03   \n",
      "\n",
      "                 OBS DATE  VALUE DATA_FLAG   UNITS        Rounded Date  \\\n",
      "0     2015-10-16 11:08:00   0.00            INCHES 2015-10-16 11:00:00   \n",
      "1     2015-10-16 23:08:00   0.00            INCHES 2015-10-16 23:00:00   \n",
      "2     2015-10-17 11:08:00   0.00            INCHES 2015-10-17 11:00:00   \n",
      "3     2015-10-17 23:08:00   0.00            INCHES 2015-10-17 23:00:00   \n",
      "4     2015-10-18 11:08:00   0.00            INCHES 2015-10-18 11:00:00   \n",
      "...                   ...    ...       ...     ...                 ...   \n",
      "10600 2024-08-25 12:03:00  47.19            INCHES 2024-08-25 12:00:00   \n",
      "10601 2024-08-26 12:03:00  47.19            INCHES 2024-08-26 12:00:00   \n",
      "10602 2024-08-27 12:03:00  47.19            INCHES 2024-08-27 12:00:00   \n",
      "10603 2024-08-29 00:03:00  47.19            INCHES 2024-08-29 00:00:00   \n",
      "10604 2024-08-30 12:03:00  47.19            INCHES 2024-08-30 12:00:00   \n",
      "\n",
      "       Rainfall  Cumulative Daily Rainfall  Total Daily Rainfall  \n",
      "0           0.0                        0.0                   0.0  \n",
      "1           0.0                        0.0                   0.0  \n",
      "2           0.0                        0.0                   0.0  \n",
      "3           0.0                        0.0                   0.0  \n",
      "4           0.0                        0.0                   0.0  \n",
      "...         ...                        ...                   ...  \n",
      "10600       0.0                        0.0                   0.0  \n",
      "10601       0.0                        0.0                   0.0  \n",
      "10602       0.0                        0.0                   0.0  \n",
      "10603       0.0                        0.0                   0.0  \n",
      "10604       0.0                        0.0                   0.0  \n",
      "\n",
      "[10605 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the absolute difference between calculated and actual rainfall\n",
    "merged_data['Difference'] = abs(merged_data['Calculated Cumulative Rainfall (inches)'] - merged_data['Daily Rainfall (inches)'])\n",
    "\n",
    "# Filter out days where the difference is greater than 0.25 inches\n",
    "filtered_data = merged_data[merged_data['Difference'] <= 0.25]\n",
    "\n",
    "# Get the dates to keep from the filtered dataset\n",
    "dates_to_keep = filtered_data['Date'].unique()\n",
    "\n",
    "# Convert 'OBS DATE' in file_16 to datetime and ensure it's compatible for filtering\n",
    "file_16['Date'] = file_16['OBS DATE'].dt.date\n",
    "\n",
    "# Filter the original 16 file to keep only rows that match the dates in 'dates_to_keep'\n",
    "filtered_file_16 = file_16[file_16['Date'].isin(dates_to_keep)].copy()  # .copy() to avoid SettingWithCopyWarning\n",
    "\n",
    "# Now, calculate cumulative hourly rainfall for each day in the filtered 16 file\n",
    "filtered_file_16['Cumulative Daily Rainfall'] = filtered_file_16.groupby('Date')['Rainfall'].cumsum()\n",
    "\n",
    "# Merge the daily rainfall back into the filtered file\n",
    "daily_totals = filtered_file_16.groupby('Date')['Rainfall'].sum().reset_index()\n",
    "daily_totals.columns = ['Date', 'Total Daily Rainfall']\n",
    "\n",
    "# Merge the daily totals into the filtered 16 file\n",
    "filtered_file_16 = pd.merge(filtered_file_16, daily_totals, on='Date')\n",
    "\n",
    "# Drop the additional 'Date' column after filtering if needed\n",
    "filtered_file_16 = filtered_file_16.drop(columns=['Date'])\n",
    "\n",
    "# Set 'Rainfall' and 'Cumulative Daily Rainfall' to 0 if 'Total Daily Rainfall' is 0\n",
    "filtered_file_16.loc[filtered_file_16['Total Daily Rainfall'] == 0, ['Rainfall', 'Cumulative Daily Rainfall']] = 0\n",
    "\n",
    "# Save the result to a CSV file\n",
    "filtered_file_16.to_csv('Resources/BLN_cleaned_hourly_Rainfall.csv', index=False)\n",
    "\n",
    "\n",
    "# Display the final dataset with cumulative hourly rainfall and total daily rainfall\n",
    "print(filtered_file_16)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SCH Station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniforge/base/envs/flooding_model/lib/python3.11/site-packages/openpyxl/styles/stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n",
      "/usr/local/Caskroom/miniforge/base/envs/flooding_model/lib/python3.11/site-packages/openpyxl/styles/stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Date  Calculated Cumulative Rainfall (inches)  \\\n",
      "0     2016-10-01                                      0.0   \n",
      "1     2016-10-02                                      0.0   \n",
      "2     2016-10-03                                      0.0   \n",
      "3     2016-10-04                                      0.0   \n",
      "4     2016-10-05                                      0.0   \n",
      "...          ...                                      ...   \n",
      "2713  2024-08-22                                      0.0   \n",
      "2714  2024-08-23                                      0.0   \n",
      "2715  2024-08-25                                      0.0   \n",
      "2716  2024-08-30                                      0.0   \n",
      "2717  2024-08-31                                      0.0   \n",
      "\n",
      "      Daily Rainfall (inches)  Discrepancy  \n",
      "0                         0.0        False  \n",
      "1                         0.0        False  \n",
      "2                         0.0        False  \n",
      "3                         0.0        False  \n",
      "4                         0.0        False  \n",
      "...                       ...          ...  \n",
      "2713                      0.0        False  \n",
      "2714                      0.0        False  \n",
      "2715                      0.0        False  \n",
      "2716                      0.0        False  \n",
      "2717                      0.0        False  \n",
      "\n",
      "[2718 rows x 4 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_d/q6dl733j0hdfhxbg883rg3xc0000gn/T/ipykernel_2662/458426343.py:9: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  file_16['Rounded Date'] = file_16['OBS DATE'].dt.round('H')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the two Excel files\n",
    "file_16 = pd.read_excel('Resources/SCH_16.xlsx')\n",
    "file_2 = pd.read_excel('Resources/SCH_2.xlsx')\n",
    "\n",
    "# Convert 'OBS DATE' in file_16 to datetime and round to nearest hour\n",
    "file_16['OBS DATE'] = pd.to_datetime(file_16['OBS DATE'])\n",
    "file_16['Rounded Date'] = file_16['OBS DATE'].dt.round('H')\n",
    "\n",
    "# Calculate rainfall increments for file 16\n",
    "file_16['Rainfall'] = file_16['VALUE'].diff().fillna(0)  # Calculate rainfall increments\n",
    "\n",
    "# Calculate daily cumulative rainfall for File 16\n",
    "file_16_daily = file_16.groupby(file_16['Rounded Date'].dt.date)['Rainfall'].sum().reset_index()\n",
    "file_16_daily.columns = ['Date', 'Calculated Cumulative Rainfall (inches)']\n",
    "\n",
    "# Convert 'OBS DATE' in file_2 to datetime\n",
    "file_2['OBS DATE'] = pd.to_datetime(file_2['OBS DATE'])\n",
    "\n",
    "# Extract relevant daily cumulative data from file 2\n",
    "file_2_daily = file_2[['OBS DATE', 'VALUE']].dropna()\n",
    "file_2_daily.columns = ['Date', 'Actual Cumulative Rainfall (inches)']\n",
    "\n",
    "# Calculate daily rainfall for File 2\n",
    "file_2_daily['Daily Rainfall (inches)'] = file_2_daily['Actual Cumulative Rainfall (inches)'].diff().fillna(0)\n",
    "\n",
    "# Handle reset condition: if current day value is less than the previous day's value, it's a reset\n",
    "file_2_daily.loc[file_2_daily['Actual Cumulative Rainfall (inches)'] < file_2_daily['Actual Cumulative Rainfall (inches)'].shift(1), \n",
    "                 'Daily Rainfall (inches)'] = file_2_daily['Actual Cumulative Rainfall (inches)']\n",
    "\n",
    "# Ensure both 'Date' columns are of the same type\n",
    "file_2_daily['Date'] = file_2_daily['Date'].dt.date\n",
    "\n",
    "# Merge the two datasets on 'Date'\n",
    "merged_data = pd.merge(file_16_daily, file_2_daily[['Date', 'Daily Rainfall (inches)']], on='Date', how='inner')\n",
    "\n",
    "# Step 4: Identify discrepancies\n",
    "merged_data['Discrepancy'] = merged_data['Calculated Cumulative Rainfall (inches)'] != merged_data['Daily Rainfall (inches)']\n",
    "\n",
    "# Display the merged data with calculated discrepancies\n",
    "print(merged_data)\n",
    "\n",
    "merged_data.to_csv('Resources/SCH_Daily_Rainfall_compare.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove data from 16 file if discrepancy is greater than .25 inches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      STATION_ID DURATION  SENSOR_NUMBER SENS_TYPE         DATE TIME  \\\n",
      "0            SCH        E             16   RAINTIP  2016-10-01 01:47   \n",
      "1            SCH        E             16   RAINTIP  2016-10-01 13:47   \n",
      "2            SCH        E             16   RAINTIP  2016-10-02 01:47   \n",
      "3            SCH        E             16   RAINTIP  2016-10-02 13:47   \n",
      "4            SCH        E             16   RAINTIP  2016-10-03 01:47   \n",
      "...          ...      ...            ...       ...               ...   \n",
      "13719        SCH        E             16   RAINTIP  2024-08-22 12:23   \n",
      "13720        SCH        E             16   RAINTIP  2024-08-23 12:23   \n",
      "13721        SCH        E             16   RAINTIP  2024-08-25 12:23   \n",
      "13722        SCH        E             16   RAINTIP  2024-08-30 12:25   \n",
      "13723        SCH        E             16   RAINTIP  2024-08-31 12:25   \n",
      "\n",
      "                 OBS DATE  VALUE DATA_FLAG   UNITS        Rounded Date  \\\n",
      "0     2016-10-01 01:47:00  43.17            INCHES 2016-10-01 02:00:00   \n",
      "1     2016-10-01 13:47:00  43.17            INCHES 2016-10-01 14:00:00   \n",
      "2     2016-10-02 01:47:00  43.17            INCHES 2016-10-02 02:00:00   \n",
      "3     2016-10-02 13:47:00  43.17            INCHES 2016-10-02 14:00:00   \n",
      "4     2016-10-03 01:47:00  43.17            INCHES 2016-10-03 02:00:00   \n",
      "...                   ...    ...       ...     ...                 ...   \n",
      "13719 2024-08-22 12:23:00  48.89            INCHES 2024-08-22 12:00:00   \n",
      "13720 2024-08-23 12:23:00  48.89            INCHES 2024-08-23 12:00:00   \n",
      "13721 2024-08-25 12:23:00  48.89            INCHES 2024-08-25 12:00:00   \n",
      "13722 2024-08-30 12:25:00  48.89            INCHES 2024-08-30 12:00:00   \n",
      "13723 2024-08-31 12:25:00  48.89            INCHES 2024-08-31 12:00:00   \n",
      "\n",
      "       Rainfall  Cumulative Daily Rainfall  Total Daily Rainfall  \n",
      "0           0.0                        0.0                   0.0  \n",
      "1           0.0                        0.0                   0.0  \n",
      "2           0.0                        0.0                   0.0  \n",
      "3           0.0                        0.0                   0.0  \n",
      "4           0.0                        0.0                   0.0  \n",
      "...         ...                        ...                   ...  \n",
      "13719       0.0                        0.0                   0.0  \n",
      "13720       0.0                        0.0                   0.0  \n",
      "13721       0.0                        0.0                   0.0  \n",
      "13722       0.0                        0.0                   0.0  \n",
      "13723       0.0                        0.0                   0.0  \n",
      "\n",
      "[13724 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the absolute difference between calculated and actual rainfall\n",
    "merged_data['Difference'] = abs(merged_data['Calculated Cumulative Rainfall (inches)'] - merged_data['Daily Rainfall (inches)'])\n",
    "\n",
    "# Filter out days where the difference is greater than 0.25 inches\n",
    "filtered_data = merged_data[merged_data['Difference'] <= 0.25]\n",
    "\n",
    "# Get the dates to keep from the filtered dataset\n",
    "dates_to_keep = filtered_data['Date'].unique()\n",
    "\n",
    "# Convert 'OBS DATE' in file_16 to datetime and ensure it's compatible for filtering\n",
    "file_16['Date'] = file_16['OBS DATE'].dt.date\n",
    "\n",
    "# Filter the original 16 file to keep only rows that match the dates in 'dates_to_keep'\n",
    "filtered_file_16 = file_16[file_16['Date'].isin(dates_to_keep)].copy()  # .copy() to avoid SettingWithCopyWarning\n",
    "\n",
    "# Now, calculate cumulative hourly rainfall for each day in the filtered 16 file\n",
    "filtered_file_16['Cumulative Daily Rainfall'] = filtered_file_16.groupby('Date')['Rainfall'].cumsum()\n",
    "\n",
    "# Merge the daily rainfall back into the filtered file\n",
    "daily_totals = filtered_file_16.groupby('Date')['Rainfall'].sum().reset_index()\n",
    "daily_totals.columns = ['Date', 'Total Daily Rainfall']\n",
    "\n",
    "# Merge the daily totals into the filtered 16 file\n",
    "filtered_file_16 = pd.merge(filtered_file_16, daily_totals, on='Date')\n",
    "\n",
    "# Drop the additional 'Date' column after filtering if needed\n",
    "filtered_file_16 = filtered_file_16.drop(columns=['Date'])\n",
    "\n",
    "# Set 'Rainfall' and 'Cumulative Daily Rainfall' to 0 if 'Total Daily Rainfall' is 0\n",
    "filtered_file_16.loc[filtered_file_16['Total Daily Rainfall'] == 0, ['Rainfall', 'Cumulative Daily Rainfall']] = 0\n",
    "\n",
    "# Save the result to a CSV file\n",
    "filtered_file_16.to_csv('Resources/SCH_cleaned_hourly_Rainfall.csv', index=False)\n",
    "\n",
    "\n",
    "# Display the final dataset with cumulative hourly rainfall and total daily rainfall\n",
    "print(filtered_file_16)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean Data, check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Empty DataFrame\n",
       " Columns: [date_hour, discharge]\n",
       " Index: [],\n",
       "       STATION_ID DURATION  SENSOR_NUMBER SENS_TYPE             OBS DATE  \\\n",
       " 1076         BLO        H              2      RAIN  2014-10-15 20:00:00   \n",
       " 1077         BLO        H              2      RAIN  2014-10-15 21:00:00   \n",
       " 1078         BLO        H              2      RAIN  2014-10-15 22:00:00   \n",
       " 1079         BLO        H              2      RAIN  2014-10-15 23:00:00   \n",
       " 1080         BLO        H              2      RAIN  2014-10-16 00:00:00   \n",
       " ...          ...      ...            ...       ...                  ...   \n",
       " 82615        BLO        H              2      RAIN  2024-02-03 16:00:00   \n",
       " 82788        BLO        H              2      RAIN  2024-02-10 21:00:00   \n",
       " 82801        BLO        H              2      RAIN  2024-02-11 10:00:00   \n",
       " 83465        BLO        H              2      RAIN  2024-03-10 03:00:00   \n",
       " 84071        BLO        H              2      RAIN  2024-04-04 09:00:00   \n",
       " \n",
       "        VALUE DATA_FLAG   UNITS  \n",
       " 1076     NaN            INCHES  \n",
       " 1077     NaN            INCHES  \n",
       " 1078     NaN            INCHES  \n",
       " 1079     NaN            INCHES  \n",
       " 1080     NaN            INCHES  \n",
       " ...      ...       ...     ...  \n",
       " 82615    NaN            INCHES  \n",
       " 82788    NaN            INCHES  \n",
       " 82801    NaN            INCHES  \n",
       " 83465    NaN            INCHES  \n",
       " 84071    NaN            INCHES  \n",
       " \n",
       " [230 rows x 8 columns],\n",
       " Empty DataFrame\n",
       " Columns: [date_hour, gage_height]\n",
       " Index: [])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the files\n",
    "file1_path = 'Resources/transformed_max_flow_data_hourly.csv'\n",
    "file2_path = 'Resources/transformed_BLO_rain_data_hourly.csv'\n",
    "file3_path = 'Resources/transformed_stream_gage_data_hourly.csv'\n",
    "\n",
    "# Read the CSV files\n",
    "max_flow_data = pd.read_csv(file1_path)\n",
    "rain_data_BLO = pd.read_csv(file2_path)\n",
    "stream_gage_data = pd.read_csv(file3_path)\n",
    "\n",
    "# Check for missing date times in each file\n",
    "missing_max_flow = max_flow_data[max_flow_data.isnull().any(axis=1)]\n",
    "missing_rain_data_BLO = rain_data_BLO[rain_data_BLO.isnull().any(axis=1)]\n",
    "missing_stream_gage = stream_gage_data[stream_gage_data.isnull().any(axis=1)]\n",
    "\n",
    "# Display any rows with missing date times\n",
    "(missing_max_flow, missing_rain_data_BLO, missing_stream_gage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells me only the rain dataset is missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "230"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of missing data rows in the rain dataset\n",
    "missing_rain_data_count_BLO = rain_data_BLO['VALUE'].isnull().sum()\n",
    "\n",
    "missing_rain_data_count_BLO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze rain data around missing values. Am I missing crucial data?  \n",
    "I know from the USGS site all of the historic gage heights that have been recorded. The greatest heights recorded during the project time period have been saved into Resources/Max_crest_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Resources/missing_rain_days.csv'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter for all the days that have any missing rain values\n",
    "missing_days_rain_BLO = rain_data_BLO[rain_data_BLO['VALUE'].isnull()]['OBS DATE'].unique()\n",
    "\n",
    "# Create a DataFrame with the missing days\n",
    "missing_days_BLO_df = pd.DataFrame(missing_days_rain_BLO, columns=['OBS DATE'])\n",
    "\n",
    "# Save the missing days to a CSV file\n",
    "missing_days_output_path = 'Resources/missing_rain_days_BLO.csv'\n",
    "missing_days_BLO_df.to_csv(missing_days_output_path, index=False)\n",
    "\n",
    "missing_days_output_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare missing data days with Max Crest Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Height (ft)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Date, Height (ft)]\n",
       "Index: []"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the two files\n",
    "max_crest_data = pd.read_csv('Resources/Max_crest_data.csv')\n",
    "missing_rain_days_BLO_data = pd.read_csv('Resources/missing_rain_days_BLO.csv')\n",
    "\n",
    "# Convert both date columns to datetime\n",
    "max_crest_data['Date'] = pd.to_datetime(max_crest_data['Date'], errors='coerce')\n",
    "missing_rain_days_BLO_data['OBS DATE'] = pd.to_datetime(missing_rain_days_BLO_data['OBS DATE'], errors='coerce')\n",
    "\n",
    "# Compare and find matching dates between the two datasets\n",
    "matching_dates = max_crest_data[max_crest_data['Date'].isin(missing_rain_days_BLO_data['OBS DATE'])]\n",
    "\n",
    "matching_dates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if any missing dates fall within 3 days (before or after) of the max crest dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Missing Date</th>\n",
       "      <th>Crest Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-03 13:00:00</td>\n",
       "      <td>2017-01-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-02-09 10:00:00</td>\n",
       "      <td>2017-02-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-03-12 03:00:00</td>\n",
       "      <td>2023-03-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-03-12 03:00:00</td>\n",
       "      <td>2023-03-14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Missing Date Crest Date\n",
       "0 2017-01-03 13:00:00 2017-01-04\n",
       "1 2017-02-09 10:00:00 2017-02-07\n",
       "2 2023-03-12 03:00:00 2023-03-10\n",
       "3 2023-03-12 03:00:00 2023-03-14"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use timedelta to create a 3-day window for comparison\n",
    "three_days_window = pd.Timedelta(days=3)\n",
    "\n",
    "# Compare the missing rain days and max crest days to see if they fall within 3 days of each other\n",
    "matching_dates_within_3_days = []\n",
    "\n",
    "for missing_date in missing_rain_days_BLO_data['OBS DATE']:\n",
    "    for crest_date in max_crest_data['Date']:\n",
    "        if abs(missing_date - crest_date) <= three_days_window:\n",
    "            matching_dates_within_3_days.append({'Missing Date': missing_date, 'Crest Date': crest_date})\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "matching_within_3_days_df = pd.DataFrame(matching_dates_within_3_days)\n",
    "\n",
    "matching_within_3_days_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OBS DATE</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-07-22</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-10-16</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-03-19</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-09-18</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-12-07</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2024-01-30</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2023-12-09</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2014-10-17</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2021-04-18</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2021-07-23</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2023-11-22</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2023-12-08</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2021-03-11</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2018-07-08</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2014-10-15</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2024-01-03</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      OBS DATE  count\n",
       "0   2021-07-22     24\n",
       "1   2014-10-16     24\n",
       "2   2015-03-19     12\n",
       "3   2023-09-18     12\n",
       "4   2023-12-07     11\n",
       "5   2024-01-30     10\n",
       "6   2023-12-09     10\n",
       "7   2014-10-17      9\n",
       "8   2021-04-18      8\n",
       "9   2021-07-23      8\n",
       "10  2023-11-22      7\n",
       "11  2023-12-08      5\n",
       "12  2021-03-11      4\n",
       "13  2018-07-08      4\n",
       "14  2014-10-15      4\n",
       "15  2024-01-03      4"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, let's count the number of missing values per day in the missing rain data\n",
    "missing_counts_per_day = missing_rain_days_BLO_data['OBS DATE'].dt.date.value_counts()\n",
    "\n",
    "# Identify days where all 24 hours are missing and days where more than 3 values are missing\n",
    "completely_missing_days = missing_counts_per_day[missing_counts_per_day == 24]\n",
    "more_than_3_missing_days = missing_counts_per_day[missing_counts_per_day > 3]\n",
    "\n",
    "# Create DataFrames for both completely missing days and more than 3 missing days\n",
    "completely_missing_days_df = pd.DataFrame(completely_missing_days).reset_index().rename(columns={'index': 'OBS Date', 'OBS Date': 'Missing Count'})\n",
    "more_than_3_missing_days_df = pd.DataFrame(more_than_3_missing_days).reset_index().rename(columns={'index': 'OBS DATE', 'OBS Date': 'Missing Count'})\n",
    "\n",
    "more_than_3_missing_days_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can only use .dt accessor with datetimelike values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Check if there are any full days with missing data\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# We will group by date (day) and check if all hourly data is missing for any of the days.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Extract just the date part from 'OBS DATE'\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m rain_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDATE\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m rain_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOBS DATE\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mdate\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Group by date and check if there are any days where all 24 hourly values are missing\u001b[39;00m\n\u001b[1;32m      8\u001b[0m missing_full_days \u001b[38;5;241m=\u001b[39m rain_data\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDATE\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVALUE\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39misnull()\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m24\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/flooding_model/lib/python3.11/site-packages/pandas/core/generic.py:6299\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   6293\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[1;32m   6294\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[1;32m   6295\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[1;32m   6296\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[1;32m   6297\u001b[0m ):\n\u001b[1;32m   6298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[0;32m-> 6299\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/flooding_model/lib/python3.11/site-packages/pandas/core/accessor.py:224\u001b[0m, in \u001b[0;36mCachedAccessor.__get__\u001b[0;34m(self, obj, cls)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m# we're accessing the attribute of the class, i.e., Dataset.geo\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessor\n\u001b[0;32m--> 224\u001b[0m accessor_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessor(obj)\n\u001b[1;32m    225\u001b[0m \u001b[38;5;66;03m# Replace the property with the accessor object. Inspired by:\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;66;03m# https://www.pydanny.com/cached-property.html\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;66;03m# We need to use object.__setattr__ because we overwrite __setattr__ on\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;66;03m# NDFrame\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name, accessor_obj)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/flooding_model/lib/python3.11/site-packages/pandas/core/indexes/accessors.py:643\u001b[0m, in \u001b[0;36mCombinedDatetimelikeProperties.__new__\u001b[0;34m(cls, data)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data\u001b[38;5;241m.\u001b[39mdtype, PeriodDtype):\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PeriodProperties(data, orig)\n\u001b[0;32m--> 643\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan only use .dt accessor with datetimelike values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can only use .dt accessor with datetimelike values"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "boot_camp2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
